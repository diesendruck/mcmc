x.values <- fitted.curves[[1]]
y.predictions <- list(fitted.curves[[2]],
fitted.curves[[3]],
fitted.curves[[4]])
x.mean <- fitted.curves[[5]][1]
y.mean <- fitted.curves[[5]][2]
num.xvalues <- length(x.values)
num.ypredictions <- length(y.predictions)
mses <- c()
# Compute sum of squared errors for all y prediction functions.
for (i in 1:num.ypredictions) {
sum.sq.err.i <- 0
pred.func.i <- y.predictions[[i]]
# For given function, compute squared errors, and add to sum.
for (j in 1:num.xvalues) {
# Mean of Y is added back to predicted, mean-subtracted Y outputs.
predicted.value <- pred.func.i[j]+y.mean
# Mean of X is added back to original mean-subtracted X inputs.
if (flag == "Smooth") {
expected.value <- (x.values[j]+x.mean)^2
sq.err <- (predicted.value - expected.value)^2
} else if (flag == "Wiggly") {
expected.value <- sin(2*pi*(x.values[j]+x.mean))
sq.err <- (predicted.value - expected.value)^2
} else {
print("Didn't find correct flag in ComputeMSE.")
}
sum.sq.err.i <- sum.sq.err.i + sq.err
}
mse.element <- (1/num.xvalues)*sum.sq.err.i
mses <- c(mses, mse.element)
}
return (mses)
}
Main()
debugSource('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R')
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R')
help(quit)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
help(quit)
help(interactive)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
df1
fitted.curves1 <- FitCurves(1000, c(0.1, 1, 4), GenerateSmoothClean)
mses1 <- ComputeMSE(fitted.curves1, flag="Smooth")
fitted.curves2 <- FitCurves(1000, c(0.1, 1, 4), GenerateSmoothNoisy)
mses2 <- ComputeMSE(fitted.curves2, flag="Smooth")
fitted.curves3 <- FitCurves(1000, c(0.1, 1, 4), GenerateWigglyClean)
mses3 <- ComputeMSE(fitted.curves3, flag="Wiggly")
fitted.curves4 <- FitCurves(1000, c(0.1, 1, 4), GenerateWigglyNoisy)
mses4 <- ComputeMSE(fitted.curves4, flag="Wiggly")
# Plot results.
d1 <- cbind(rep("Smooth Clean",3), c("0.1","1","4"), mses1)
d2 <- cbind(rep("Smooth Noisy",3), c("0.1","1","4"), mses2)
d3 <- cbind(rep("Wiggly Clean",3), c("0.1","1","4"), mses3)
d4 <- cbind(rep("Wiggly Noisy",3), c("0.1","1","4"), mses4)
df1 <- data.frame(rbind(d1, d2, d3, d4), stringsAsFactors=FALSE)
names(df1) <- c("Case","Bandwidth","Value")
df1[,3] <- sapply(df1[,3], as.numeric)
df1
View(df1)
View(df1)
rbind(mses1, mses2, mses3, mses4)
a <- rbind(mses1, mses2, mses3, mses4)
names(a) <- c("0.1", "1", "4")
a
df1
a
df2 <- data.frame(a)
df2
names(df2) <- c("0.1", "1", "4")
df2
row.names(df) <- c("Smooth Clean", "Smooth Noisy", "Wiggly Clean", "Wiggly Noisy")
row.names(df2) <- c("Smooth Clean", "Smooth Noisy", "Wiggly Clean", "Wiggly Noisy")
df2
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
fitted.curves1 <- FitCurves(1000, c(0.1, 1, 4), GenerateSmoothClean)
mses1 <- ComputeMSE(fitted.curves1, flag="Smooth")
mses1
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p3.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
x.input <- seq(0, 1, 500)
x.input
x.input <- seq(0, 1, length=500)
x.input
fitted.curves3 <- FitCurves(1000, c(0.1, 1, 4), GenerateWigglyClean)
GenerateWigglyClean <- function(n) {
# Takes set of non-linear X's, applies a function, and adds noise.
#
# Args:
#   n: Number of pairs to be generated.
#
# Returns:
#   data: List, with mean-subtracted pairs of inputs X and outputs Y, and
#     with vector of means.
x.input <- rbeta(n, 1, 5)
x.input <- seq(0, 1, length=500)
errors <- rnorm(n, 0, 0.01)
y.output <- sin(2*pi*x.input) + errors
# Subract out means.
ms.input <- x.input - mean(x.input)
ms.output <- y.output - mean(y.output)
ms.pairs <- cbind(ms.output, ms.input)
pairs <- cbind(y.output, x.input)
data <- list(ms.pairs, c(mean(x.input), mean(y.output)), pairs)
return (data)
}
fitted.curves3 <- FitCurves(1000, c(0.1, 1, 4), GenerateWigglyClean)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
GenerateWigglyNoisy <- function(n) {
# Takes set of non-linear X's, applies a function, and adds noise.
#
# Args:
#   n: Number of pairs to be generated.
#
# Returns:
#   data: List, with mean-subtracted pairs of inputs X and outputs Y, and
#     with vector of means.
x.input <- rbeta(n, 1, 5)
x.input <- seq(0, 1, length=500) # Added later.
errors <- rnorm(n, 0, 1)
y.output <- sin(2*pi*x.input) + errors
# Subract out means.
ms.input <- x.input - mean(x.input)
ms.output <- y.output - mean(y.output)
ms.pairs <- cbind(ms.output, ms.input)
pairs <- cbind(y.output, x.input)
data <- list(ms.pairs, c(mean(x.input), mean(y.output)), pairs)
return (data)
}
fitted.curves4 <- FitCurves(1000, c(0.1, 1, 4), GenerateWigglyNoisy)
source('~/Google Drive/2. SPRING 2015/STAT MOD 2 - Prof Scott/exe2 - smoothing - p4.R', echo=TRUE)
Main <- function() {
# Set up data.
setwd("~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3")
contents <- read.csv("hw3.2.csv", header=T, sep=",")
x <- as.matrix(contents[,c(1,2)])
x <- cbind(1, x)
y <- as.matrix(contents[,3])
# Run Gibbs Sampler.
iter <- 100
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
# Do traceplots and diagnostics for B.
num.betas <- dim(x)[2]
par(mfrow=c(num.betas,2))
burnin <- 20
estimate.b <- matrix(NA, nrow=num.betas, ncol=1)
for (i in 1:num.betas) {
hist(b.chain[i, ], 40)
plot(ts(b.chain[i, burnin:iter]))
estimate <- mean(b.chain[i, burnin:iter])
estimate.b[i, ] <- estimate
}
# Do traceplots and diagnostics for Z.
n <- length(y)
for (i in 1:n) {
burnin <- 20
#plot(ts(z.chain[i, burnin:iter]), main=c("z", i), ylim=c(-3, 3))
}
estimate.z <- matrix(NA, nrow=n, ncol=1)
for (j in 1:n) {
estimate.z[j, ] <- mean(z.chain[j, burnin:iter])
}
# Final estimates for B and Z.
estimate.b
estimate.z
}
GibbsSampler <- function(x, y, iter) {
# Performs Gibbs sample to simulate joint posterior P(B,z|y).
#
# Args:
#   x: Design matrix.
#   y: Response vector.
#   iter: Number of iterations.
#
# Returns:
#   chains: Chains of estimated B and z random variables.
num.betas <- dim(x)[2]
n <- dim(x)[1]
# Set starting values for beta (1's) and z (NA's) chains.
b.chain <- matrix(1, nrow=num.betas, ncol=iter)
z.chain <- matrix(0, nrow=n, ncol=iter)
# Initialize current variables with first elements of chains.
b.i <- b.chain[,1]
z.i <- z.chain[,1]
for (i in 1:iter) {
new.vars <- OneGS(b.i, z.i, n, x, y)
b.i <- new.vars[[1]]
z.i <- new.vars[[2]]
b.chain[, i] <- b.i
z.chain[, i] <- z.i
}
chains <- list(b.chain, z.chain)
return (chains)
}
OneGS <- function(b.i, z.i, n, x, y) {
# Runs one loop of Gibbs Sampler and returns new versions of B and Z.
#
# Args:
#   b.i: Current beta vector.
#   z.i: Current Z vector.
#   n: Number of iterations.
#   x: Design matrix.
#   y: Response vector.
#
# Returns:
#   output: List of updated beta and Z.
new.z <- matrix(0, nrow=n, ncol=1)
# Sample for Z's, given current betas.
for (i in 1:n) {
if (y[i]==1) {
new.z[i,] <- rtruncnorm(1, a=0, b=Inf, mean=x[i,]%*%b.i, sd=1)
} else if (y[i]==0) {
new.z[i,] <- rtruncnorm(1, a=-Inf, b=0, mean=x[i,]%*%b.i, sd=1)
}
}
z.i <- new.z
# Sample for B, given new Z's.
b.i <- mvrnorm(1, mu=solve(t(x)%*%x)%*%t(x)%*%z.i, Sigma=solve(t(x)%*%x))
output <- list(b.i, z.i)
return (output)
}
Main()
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
b.chain <- chains[[1]]
chains <- GibbsSampler(x, y, iter)
setwd("~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3")
contents <- read.csv("hw3.2.csv", header=T, sep=",")
x <- as.matrix(contents[,c(1,2)])
x <- cbind(1, x)
y <- as.matrix(contents[,3])
# Run Gibbs Sampler.
iter <- 500
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
dim(b.chain)
b.chain <- b.chain[, seq(1:length(b.chain), 5)]
b.chain <- chains[[1]]
dim(b.chain)
b.chain <- b.chain[, seq(1, length(b.chain), 5)]
seq(1, length(b.chain), 5)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
# Thin each chain by extracting every 5th element.
dim(b.chain)
b.chain <- b.chain[, seq(1, dim(b.chain)[2], 5)]
dim(b.chain)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
# MCMC - HW3.2
library(truncnorm)
library(MASS)
# RESOURCES
# http://en.wikipedia.org/wiki/Probit_model
# http://book.isito.kg/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0/%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5/R/Albert%20J.%20-%20Bayesian%20Computation%20with%20R,%202e%20-%202009.pdf
# http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mcmc_sect053.htm
# http://fisher.osu.edu/~schroeder.9/AMIS900/ech7.pdf
# http://www.stat.cmu.edu/~brian/905-2009/all-papers/albert-chib-1993.pdf
Main <- function() {
# Set up data.
setwd("~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3")
contents <- read.csv("hw3.2.csv", header=T, sep=",")
x <- as.matrix(contents[,c(1,2)])
x <- cbind(1, x)
y <- as.matrix(contents[,3])
# Run Gibbs Sampler.
iter <- 500
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
# Thin each chain by extracting every 5th element.
b.chain <- b.chain[, seq(1, dim(b.chain)[2], 5)]
z.chain <- z.chain[, seq(1, dim(z.chain)[2], 5)]
# Do traceplots and diagnostics for B.
burnin <- 100
len <- dim(b.chain)[2]
num.betas <- dim(x)[2]
par(mfrow=c(num.betas,2))
estimate.b <- matrix(NA, nrow=num.betas, ncol=1)
for (i in 1:num.betas) {
hist(b.chain[i, ], 40, xlab="Beta Value", ylab="Frequency",
main=paste("Histogram of Beta",i,"'s"))
plot(ts(b.chain[i, burnin:len]), xlab="Iteration", ylab="Beta Value",
main=paste("Mixing for Beta",i))
estimate <- mean(b.chain[i, burnin:len])
estimate.b[i, ] <- estimate
}
# Do traceplots and diagnostics for Z.
n <- length(y)
for (i in 1:n) {
burnin <- 20
#plot(ts(z.chain[i, burnin:iter]), main=c("z", i), ylim=c(-3, 3))
}
estimate.z <- matrix(NA, nrow=n, ncol=1)
for (j in 1:n) {
estimate.z[j, ] <- mean(z.chain[j, burnin:len])
}
# Final estimates for B and Z.
estimate.b
estimate.z
}
GibbsSampler <- function(x, y, iter) {
# Performs Gibbs sample to simulate joint posterior P(B,z|y).
#
# Args:
#   x: Design matrix.
#   y: Response vector.
#   iter: Number of iterations.
#
# Returns:
#   chains: Chains of estimated B and z random variables.
num.betas <- dim(x)[2]
n <- dim(x)[1]
# Set starting values for beta (1's) and z (NA's) chains.
b.chain <- matrix(1, nrow=num.betas, ncol=iter)
z.chain <- matrix(0, nrow=n, ncol=iter)
# Initialize current variables with first elements of chains.
b.i <- b.chain[,1]
z.i <- z.chain[,1]
for (i in 1:iter) {
new.vars <- OneGS(b.i, z.i, n, x, y)
b.i <- new.vars[[1]]
z.i <- new.vars[[2]]
b.chain[, i] <- b.i
z.chain[, i] <- z.i
}
chains <- list(b.chain, z.chain)
return (chains)
}
OneGS <- function(b.i, z.i, n, x, y) {
# Runs one loop of Gibbs Sampler and returns new versions of B and Z.
#
# Args:
#   b.i: Current beta vector.
#   z.i: Current Z vector.
#   n: Number of iterations.
#   x: Design matrix.
#   y: Response vector.
#
# Returns:
#   output: List of updated beta and Z.
new.z <- matrix(0, nrow=n, ncol=1)
# Sample for Z's, given current betas.
for (i in 1:n) {
if (y[i]==1) {
new.z[i,] <- rtruncnorm(1, a=0, b=Inf, mean=x[i,]%*%b.i, sd=1)
} else if (y[i]==0) {
new.z[i,] <- rtruncnorm(1, a=-Inf, b=0, mean=x[i,]%*%b.i, sd=1)
}
}
z.i <- new.z
# Sample for B, given new Z's.
b.i <- mvrnorm(1, mu=solve(t(x)%*%x)%*%t(x)%*%z.i, Sigma=solve(t(x)%*%x))
output <- list(b.i, z.i)
return (output)
}
setwd("~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3")
contents <- read.csv("hw3.2.csv", header=T, sep=",")
x <- as.matrix(contents[,c(1,2)])
x <- cbind(1, x)
y <- as.matrix(contents[,3])
# Run Gibbs Sampler.
iter <- 500
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
b.chain <- b.chain[, seq(1, dim(b.chain)[2], 5)]
z.chain <- z.chain[, seq(1, dim(z.chain)[2], 5)]
burnin <- 10
len <- dim(b.chain)[2]
num.betas <- dim(x)[2]
par(mfrow=c(num.betas,2))
estimate.b <- matrix(NA, nrow=num.betas, ncol=1)
for (i in 1:num.betas) {
hist(b.chain[i, ], 40, xlab="Beta Value", ylab="Frequency",
main=paste("Histogram of Beta",i,"'s"))
plot(ts(b.chain[i, burnin:len]), xlab="Iteration", ylab="Beta Value",
main=paste("Mixing for Beta",i))
estimate <- mean(b.chain[i, burnin:len])
estimate.b[i, ] <- estimate
}
Main <- function() {
# Set up data.
setwd("~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3")
contents <- read.csv("hw3.2.csv", header=T, sep=",")
x <- as.matrix(contents[,c(1,2)])
x <- cbind(1, x)
y <- as.matrix(contents[,3])
# Run Gibbs Sampler.
iter <- 1000
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
# Thin each chain by extracting every 5th element.
b.chain <- b.chain[, seq(1, dim(b.chain)[2], 5)]
z.chain <- z.chain[, seq(1, dim(z.chain)[2], 5)]
# Do traceplots and diagnostics for B.
burnin <- 10
len <- dim(b.chain)[2]
num.betas <- dim(x)[2]
par(mfrow=c(num.betas,2))
estimate.b <- matrix(NA, nrow=num.betas, ncol=1)
for (i in 1:num.betas) {
hist(b.chain[i, ], 40, xlab="Beta Value", ylab="Frequency",
main=paste("Histogram of Beta",i,"'s"))
plot(ts(b.chain[i, burnin:len]), xlab="Iteration", ylab="Beta Value",
main=paste("Mixing for Beta",i))
estimate <- mean(b.chain[i, burnin:len])
estimate.b[i, ] <- estimate
}
# Do traceplots and diagnostics for Z.
n <- length(y)
for (i in 1:n) {
burnin <- 20
#plot(ts(z.chain[i, burnin:iter]), main=c("z", i), ylim=c(-3, 3))
}
estimate.z <- matrix(NA, nrow=n, ncol=1)
for (j in 1:n) {
estimate.z[j, ] <- mean(z.chain[j, burnin:len])
}
# Final estimates for B and Z.
estimate.b
estimate.z
}
iter <- 1000
chains <- GibbsSampler(x, y, iter)
b.chain <- chains[[1]]
z.chain <- chains[[2]]
# Thin each chain by extracting every 5th element.
b.chain <- b.chain[, seq(1, dim(b.chain)[2], 5)]
z.chain <- z.chain[, seq(1, dim(z.chain)[2], 5)]
# Do traceplots and diagnostics for B.
burnin <- 10
len <- dim(b.chain)[2]
num.betas <- dim(x)[2]
par(mfrow=c(num.betas,2))
estimate.b <- matrix(NA, nrow=num.betas, ncol=1)
for (i in 1:num.betas) {
hist(b.chain[i, ], 40, xlab="Beta Value", ylab="Frequency",
main=paste("Histogram of Beta",i,"'s"))
plot(ts(b.chain[i, burnin:len]), xlab="Iteration", ylab="Beta Value",
main=paste("Mixing for Beta",i))
estimate <- mean(b.chain[i, burnin:len])
estimate.b[i, ] <- estimate
}
source('~/.active-rstudio-document', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
source('~/Google Drive/2. SPRING 2015/MCMC - Prof Mueller/hw3/hw3.2.R', echo=TRUE)
